{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal componen analysis\n",
    "\n",
    "\n",
    "\n",
    "## What is principal component analysis?\n",
    "\n",
    "Like the name says, principal component analysis is an statistical procedure that uses an orthogonal transformation to convert a set of variables that are correlated into one \"class\".\n",
    "\n",
    "The set of all different classes are the ones linearly uncorrelated is called the set of principal components.\n",
    "\n",
    "![alt text](https://galaxydatatech.com/wp-content/uploads/2018/07/PCA-Principal-Component-Analysis.png \"Principal Components\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations\n",
    "import numpy as np\n",
    "# Standarization of the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# Loss function\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### 2 - Data Setup\n",
    "\n",
    "Lets suppouse we have a training set and test set given by the train_test_split function from sklearn.\n",
    "\n",
    "If we have another kind of data, you can use one of the multiple options described in that archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3 - Standarization \n",
    "First you need to do standarization to the training data. After that you can do the same to the test set.\n",
    "\n",
    "If you do it all together and then split it, you mix information getting some fake positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid token (<ipython-input-4-3567dfa54382>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-3567dfa54382>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tol = 1e-number # convergence tolerance\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid token\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only\n",
    "scaler.fit(train_set)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train = scaler.transform(train_set)\n",
    "test = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Model\n",
    "Using the PCA function from sklearn the construction of the model is direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instance\n",
    "\n",
    "pca = PCA(.95) # Variance\n",
    "pca.fit(train)\n",
    "\n",
    "train = pca.transform(train)\n",
    "test = pca.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you start to apply you model to this transformation of the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
