{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "## What is naive Bayes?\n",
    "\n",
    "Naive Bayes is mostly used as a classifier. It's obviusly based in the Bayes' theorem combined with the independence (we suppouse this) of the variables.\n",
    "\n",
    "We are talking about a probabilistic algorithm, so the answer of the output will be the probabilitis of something to happend. \n",
    "\n",
    "![alt text](https://i.stack.imgur.com/QiTPe.png \"Gaussian Distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations\n",
    "import numpy as np\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# In case you want to split the dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "# We want to measure the accuracy of the model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### 2 - Data Setup\n",
    "\n",
    "We will supose that we already read the data with our csv library like we explained in the README.md\n",
    "\n",
    "If we have another kind of data, you can use one of the multiple options described in that archive.\n",
    "\n",
    "Lets suppouse we have x, y our dataset matrix made by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb17452fe5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnd_number\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 70% training and 30% test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text.data, text.target, test_size=0.3,random_state=rnd_number) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Naive Bayes\n",
    "The naive Bayes functions are implemented in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # If your distribution is a multi\n",
    "from sklearn.naive_bayes import GaussianNB # If you need a gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4 - Model\n",
    "If we are using sklearn it's really easy to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Predict\n",
    "Just like the training part, testing is a short peace of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eaf956d1e28e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict the response for test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Model Accuracy (a.k.a. norm between y_pred and the real data y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Accuracy (a.k.a. norm between y_pred and the real data y_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * - Laplace\n",
    "\n",
    "The well-known Zero Probability problem appears when we have a non-continuous set of posibilities. Theoretically, the distributions we are working with are continuous, but in the practices we have a finite number of data. This makes lot of gaps with 0 prob.\n",
    "\n",
    "The solution to this problem is adding some new \"invented\" data to fill those gaps with a tiny prob that doesn't affect the hole model.\n",
    "\n",
    "We will assume that we are trying to solve a natural languaje problem. We already defined the classes for each word, the vocabulary,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-23a7b05d9f5c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-23a7b05d9f5c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def laplace_smoothing(self, X_train, y_train, lambda):\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " def laplace_smoothing(self, X_train, y_train, lambda):\n",
    "        '''\n",
    "            apply laplace smoothing, default lunda = 1 = add one smoothing\n",
    "            on the posterior probability table\n",
    "            lunda should <= 1\n",
    "        '''\n",
    "        for classes in all_classes:\n",
    "            for word in self.vocabulary(X_train):\n",
    "                count = self.count_word_in_classes[classes][word]\n",
    "                self.similar[classes][word] = np.log((count + lamnda) / (words_in_class + lambda*len(self.vocabulary(X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch\n",
    "Code from\n",
    "https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    " \n",
    "def loadCsv(filename):\n",
    "\tlines = csv.reader(open(filename, \"rb\"))\n",
    "\tdataset = list(lines)\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tdataset[i] = [float(x) for x in dataset[i]]\n",
    "\treturn dataset\n",
    " \n",
    "def splitDataset(dataset, splitRatio):\n",
    "\ttrainSize = int(len(dataset) * splitRatio)\n",
    "\ttrainSet = []\n",
    "\tcopy = list(dataset)\n",
    "\twhile len(trainSet) < trainSize:\n",
    "\t\tindex = random.randrange(len(copy))\n",
    "\t\ttrainSet.append(copy.pop(index))\n",
    "\treturn [trainSet, copy]\n",
    " \n",
    "def separateByClass(dataset):\n",
    "\tseparated = {}\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tif (vector[-1] not in separated):\n",
    "\t\t\tseparated[vector[-1]] = []\n",
    "\t\tseparated[vector[-1]].append(vector)\n",
    "\treturn separated\n",
    " \n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    " \n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "\treturn math.sqrt(variance)\n",
    " \n",
    "def summarize(dataset):\n",
    "\tsummaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "\tdel summaries[-1]\n",
    "\treturn summaries\n",
    " \n",
    "def summarizeByClass(dataset):\n",
    "\tseparated = separateByClass(dataset)\n",
    "\tsummaries = {}\n",
    "\tfor classValue, instances in separated.iteritems():\n",
    "\t\tsummaries[classValue] = summarize(instances)\n",
    "\treturn summaries\n",
    " \n",
    "def calculateProbability(x, mean, stdev):\n",
    "\texponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "\treturn (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n",
    " \n",
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "\tprobabilities = {}\n",
    "\tfor classValue, classSummaries in summaries.iteritems():\n",
    "\t\tprobabilities[classValue] = 1\n",
    "\t\tfor i in range(len(classSummaries)):\n",
    "\t\t\tmean, stdev = classSummaries[i]\n",
    "\t\t\tx = inputVector[i]\n",
    "\t\t\tprobabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "\treturn probabilities\n",
    "\t\t\t\n",
    "def predict(summaries, inputVector):\n",
    "\tprobabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "\tbestLabel, bestProb = None, -1\n",
    "\tfor classValue, probability in probabilities.iteritems():\n",
    "\t\tif bestLabel is None or probability > bestProb:\n",
    "\t\t\tbestProb = probability\n",
    "\t\t\tbestLabel = classValue\n",
    "\treturn bestLabel\n",
    " \n",
    "def getPredictions(summaries, testSet):\n",
    "\tpredictions = []\n",
    "\tfor i in range(len(testSet)):\n",
    "\t\tresult = predict(summaries, testSet[i])\n",
    "\t\tpredictions.append(result)\n",
    "\treturn predictions\n",
    " \n",
    "def getAccuracy(testSet, predictions):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(testSet)):\n",
    "\t\tif testSet[i][-1] == predictions[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn (correct/float(len(testSet))) * 100.0\n",
    " \n",
    "def main():\n",
    "\tfilename = 'pima-indians-diabetes.data.csv'\n",
    "\tsplitRatio = 0.67\n",
    "\tdataset = loadCsv(filename)\n",
    "\ttrainingSet, testSet = splitDataset(dataset, splitRatio)\n",
    "\tprint('Split {0} rows into train={1} and test={2} rows').format(len(dataset), len(trainingSet), len(testSet))\n",
    "\t# prepare model\n",
    "\tsummaries = summarizeByClass(trainingSet)\n",
    "\t# test model\n",
    "\tpredictions = getPredictions(summaries, testSet)\n",
    "\taccuracy = getAccuracy(testSet, predictions)\n",
    "\tprint('Accuracy: {0}%').format(accuracy)\n",
    " \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
