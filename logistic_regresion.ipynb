{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "\n",
    "## What is Logistic regression?\n",
    "\n",
    "Logistic regression consist in a finite number of funtion that are \"close\" to the given data. It's an aproximation from a set of data to a logistic function. The way we measure this distance it's called a mathamatical distance and we want to minimize it without losing information.\n",
    "\n",
    "The aproximation will divide the space into 2 different parts separeted. In case you want to use this method, you need to be sure that the data can be divided like that.\n",
    "\n",
    "![alt text](https://codesachin.files.wordpress.com/2015/08/linearly_separable_4.png \"Hyperplane separation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations\n",
    "import numpy as np\n",
    "# Errors\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### 2 - Data Setup\n",
    "\n",
    "We will supose that we already read the data with our panda library like we explained in the README.md\n",
    "\n",
    "If we have another kind of data, you can use one of the multiple options described in that archive.\n",
    "\n",
    "Lets suppouse we have x, y and z as our dataset matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Logistic function\n",
    "It's alread defined  in almost every optimization librery, but there you have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic function\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid function of x.'''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4 - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid token (<ipython-input-4-3567dfa54382>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-3567dfa54382>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tol = 1e-number # convergence tolerance\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid token\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-number # convergence tolerance\n",
    "l2 = None # l2-regularization\n",
    "max_iter = number # maximum iterations during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Preventive Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for errors or non-derivative points\n",
    "def catch_singularity(f):\n",
    "    '''Change the LinAlgError for a warning.'''\n",
    "    \n",
    "    def silencer(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn('Algorithm terminated')\n",
    "            return args[0]\n",
    "    return silencer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Overfitting\n",
    "Below we implement one step of Newton's method trying to reach the l2-regularization optimal. You can try to repeat the process, but remember that the calculation cost get higher really quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def newton_step(curr, X, l2=None):\n",
    "    '''One step of Newton's Method'''\n",
    "    \n",
    "    ## Calculate aux objects\n",
    "    # Create probability matrix, miniminum \"ndmin\" dimensions\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=ndmin).T\n",
    "    # Create weight matrix from it\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    # Compute the hessian\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    # Compute the gradient\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## Regularization\n",
    "    if l2:\n",
    "        # Return the least-squares solution to a linear matrix equation\n",
    "        step, *_ = np.linalg.lstsq(hessian + lam*np.eye(curr.shape[0]), grad)\n",
    "    else:\n",
    "        step, *_ = np.linalg.lstsq(hessian, grad)\n",
    "        \n",
    "    ## Update the \"optumal\"\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Model\n",
    "\n",
    "We have to create a function that defines the distance between our aproximation on each iter and the actual solution for the given points. (In this case we used the common abs norm)\n",
    "\n",
    "#### 7.1 - Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coefs_convergence(beta_old, beta_new, tol, iters):\n",
    "    '''Check the convergence of the aprox solution'''\n",
    "    # Calculate the distance\n",
    "    coef_dis = np.abs(beta_old - beta_new)\n",
    "    # If the tol is not good enough, repeat the process\n",
    "    return not (np.any(coef_change>tol) & (iters < max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 - Iteration (Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92a30ab845d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create beta variables, one to be fixed and the other to be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbeta_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create an iteration counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0miter_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Create beta variables, one to be fixed and the other to be updated\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "\n",
    "# Create an iteration counter\n",
    "iter_count = 0\n",
    "\n",
    "# Create a convergence \"counter\"\n",
    "coefs_converged = False\n",
    "\n",
    "# Start the loop\n",
    "while not coefs_converged:\n",
    "    \n",
    "    # Update our beta\n",
    "    beta_old = beta\n",
    "    # Apply Newton methods\n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    # Update iteration counter\n",
    "    iter_count += 1\n",
    "    \n",
    "    # Check if we reached the convergence desired\n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
